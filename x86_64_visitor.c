#include <assert.h>
#include <string.h>
#include <time.h>
#include "visitor.h"
#include "types.h"
#include "stdio.h"
#include "common.h"

// every value has a PERMANENT location that is not in a register.
typedef enum {
  LOC_NONE = 0,
  // these locations represent the user's program variables or constants
  LOC_IMMEDIATE,
  LOC_STACK,
  LOC_GLOBAL,
  // these derived locations are generated by the codegen
  LOC_INDEXED,
} LocationKind;

typedef struct {
  struct x86_64_Value *base;
  struct x86_64_Value *index_expr;
  int index_const;
  int scale;
} Index;

#define IS_CONST_INDEX(index) !(index).index_expr
#define IS_SCALED_INDEX(index) ((index).scale > 0)

typedef struct x86_64_Value {
  const Type *type;
  LocationKind location_kind;
  union {
    int rbp_offset;  // for LOC_STACK
    const char *global_name;  // for LOC_GLOBAL
    int64_t integer_immediate;  // for LOC_IMMEDIATE
    double float_immediate;  // ditto
    Index index;
  };
  int in_accum;  // if it is TEMPORARILY in the accumulator
  int in_flags;  // ... or flags register
  const char *debug_name;
} x86_64_Value;


typedef struct {
  Visitor _visitor;
  // current contents
  x86_64_Value *flags_contents;
  x86_64_Value *accum_contents;  // are they the same now?
  int curr_rbp_offset;
  int curr_temp_id;
  int curr_func_param;
  FILE *out;  // should be a memstream
  const Type *curr_func_return_type;
} x86_64_Visitor;

const Type *type_of(const x86_64_Value *val) {
  return val->type;
}

static int total_size(const Type *type) {
  // TODO: Support variable sized arrays, and structs with flexible members
  return type->kind == TY_ARRAY ? type->size * total_size(type->child_type) : type->size;
}

static int child_size(const Type *type) {
  assert(type->child_type && "child_size called on an object without a child_type!");
  return total_size(type->child_type);
}

static int align(const Type *type) {
  if (IS_SCALAR_TYPE(type)) {
    return type->size;
  }
  if (type->kind == TY_ARRAY) {
    return align(type->child_type);
  }
  THROWF(EXC_INTERNAL, "Unsupported type kind %d", type->kind);
}

// generalize to more registers
static char *accum_register(int size) {
  switch (size) {
    case 1:
      return "%al";
    case 2:
      return "%ax";
    case 4:
      return "%eax";
    case 8:
      return "%rax";
    default:
      THROWF(EXC_INTERNAL, "Unsupported value size %d", size);
  }
}

/*
static char *other_register(int size) {
  switch (size) {
    case 1:
      return "%dl";
    case 2:
      return "%dx";
    case 4:
      return "%edx";
    case 8:
      return "%rdx";
    default:
      THROWF(EXC_INTERNAL, "Unsupported value size %d", size);
  }
}
*/

static char suffixes[] = {
  [1] = 'b',
  [2] = 's',
  [4] = 'l',
  [8] = 'q',
};

static const char param_registers[][6][5] = {
  [1] = {"%dil", "%sil", "%dl", "%cl", "%r8b", "%r9b"},
  [2] = {"%di", "%si", "%dx", "%cx", "%r8w", "%r9w"},
  [4] = {"%edi", "%esi", "%edx", "%ecx", "%r8d", "%r9d"},
  [8] = {"%rdi", "%rsi", "%rdx", "%rcx", "%r8", "%r9"},
};

static const char *operator(char *op, int size) {
  return fmtstr("%s%c", op, suffixes[size]);
}

static const char *addr(x86_64_Visitor *v, x86_64_Value *val);

// THIS ONLY SUPPORTS 32 BIT INDICES
static const char *indexed_addr(x86_64_Visitor *v, x86_64_Value *val) {
  x86_64_Value *base_val = val->index.base;
  
  // If we have a const index, convert it to bytes if we use the scale format. Otherwise, copy the index value to %rax
  // so we can address it via (%base,%rcx).
  int const_offset = -1;
  if (IS_CONST_INDEX(val->index)) {
    int scale = IS_SCALED_INDEX(val->index) ? val->index.scale : 1;
    const_offset = val->index.index_const * scale;
    val->index.scale = 0;
  } else {
    assert(val->type->kind == TY_INTEGER && val->type->size == 4);
    fprintf(v->out, "\tmovslq\t%s, %%rcx\n", addr(v, val->index.index_expr));
  }

  // If base is on the stack, then we use -offset(%base. Otherwise, we put it in %r10. If we have a const index (now
  // converted to an offset in bytes), put that in the displacement. Otherwise, read the index value from %rcx.
  const char *prefix, *suffix;
  switch (base_val->location_kind) {
    case LOC_STACK:
      if (const_offset >= 0) {
        prefix = fmtstr("%d(%%rbp", base_val->rbp_offset + const_offset);
      } else {
        prefix = fmtstr("%d(%%rbp,%%rcx", base_val->rbp_offset);
      }
      break;
    case LOC_GLOBAL:
      fprintf(v->out, "\tmovq\t%s, %%r10\n", base_val->global_name);
      if (const_offset >= 0) {
        prefix = fmtstr("%d(%%r10", const_offset);
      } else {
        prefix = "(%%r10,%%rcx";
      }
      break;
    default:
      raise(SIGTRAP);
      THROWF(EXC_INTERNAL, "Unsupported location %d", base_val->location_kind);
  }

  // Finally, scale the address. The interpretation of index (whether it represents bytes or elements) depends on
  // whether the index value is "scaled."
  if (IS_SCALED_INDEX(val->index)) {
    suffix = fmtstr(",%d)", val->index.scale);
  } else {
    suffix = ")";
  }

  return fmtstr("%s%s", prefix, suffix);
}

static const char *addr(x86_64_Visitor *v, x86_64_Value *val) {
  switch (val->location_kind) {
    case LOC_IMMEDIATE:
      return fmtstr("$%lld", val->integer_immediate);
    case LOC_STACK:
      return fmtstr("%d(%%rbp)", val->rbp_offset);
    case LOC_GLOBAL:
      return val->global_name;
    case LOC_INDEXED:
      return indexed_addr(v, val);
    default:
      THROWF(EXC_INTERNAL, "Unsupported location %d", val->location_kind);
  }
}

static char *BINARY_TEMPLATE = "\t%s\t%s, %s\t\t# %s = %s\n";

/**
 * Allocate a temporary variable holding a value of type on the stack. The contents are still UNDEFINED and must be
 * filled in subsequent instructions.
 *
 * This function does NOT modify rsp; that will be done right before the subroutine call (if any). Further, the stack
 * is not required to be aligned.
 */

static x86_64_Value *new_variable(x86_64_Visitor *v, const Type *type, const char *debug_name) {
  int size = total_size(type);
  // TODO: deal with alignment
  v->curr_rbp_offset -= size;
  v->curr_temp_id++;

  x86_64_Value *ret = checked_calloc(1, sizeof(x86_64_Value));
  ret->location_kind = LOC_STACK;
  ret->rbp_offset = v->curr_rbp_offset;
  ret->type = type;
  ret->debug_name = debug_name ? debug_name : fmtstr("t%d", v->curr_temp_id);

  fprintf(
    v->out,
    "\tsubq\t$%d, %%rsp\t\t# alloc %s (%d bytes) at %d(%%rbp) \n",
    size,
    ret->debug_name,
    size,
    ret->rbp_offset
  );
  return ret;
}

/*
const Type *indexed_base_type(x86_64_Value *val) {
  assert(val->location_kind == LOC_INDEXED);
  const Type *type = val->index.base->type;
  while (!IS_SCALAR_TYPE(type)) {
    type = type->child_type;
  }
  return type;
}

static int scalar_size(x86_64_Value *val) {
  if (val->location_kind == LOC_INDEXED) {
    return indexed_base_type(val)->size;
  }
  assert(IS_SCALAR_TYPE(val->type));
  return val->type->size;
}
*/

static void copy_from_accum(x86_64_Visitor *v, x86_64_Value *val) {
  assert(IS_SCALAR_TYPE(val->type));
  // update
  v->accum_contents = val;
  val->in_accum = 1;

  int size = val->type->size;
  fprintf(
    v->out,
    BINARY_TEMPLATE,
    operator("mov", size), accum_register(size), addr(v, val),
    fmtstr("%s <%s>", val->debug_name, addr(v, val)),
    accum_register(size)
  );
}

static void copy_to_accum(x86_64_Visitor *v, x86_64_Value *val) {
  assert(IS_SCALAR_TYPE(val->type));
  if (v->accum_contents == val)  // no-op
    return;

  // evict the previous contents
  if (v->accum_contents) {
    v->accum_contents->in_accum = 0;
  }

  v->accum_contents = val;
  val->in_accum = 1;

  int size = val->type->size;
  fprintf(
    v->out,
    BINARY_TEMPLATE,
    operator("mov", size), addr(v, val), accum_register(size),
    accum_register(size),
    val->debug_name
  );
}

static Type INTEGER_LITERAL_TYPE = {
  .kind = TY_INTEGER,
  .size = 4,
  .align = 4,
};

/*
static Type INT64_LITERAL_TYPE = {
  .kind = TY_INTEGER,
  .size = 8,
  .align = 8,
};
*/

static x86_64_Value *visit_integer_literal(x86_64_Visitor *v, int64_t int64_val) {
  x86_64_Value *ret = checked_calloc(1, sizeof(x86_64_Value));
  ret->location_kind = LOC_IMMEDIATE;
  ret->integer_immediate = int64_val;
  ret->debug_name = addr(v, ret);
  ret->type = &INTEGER_LITERAL_TYPE;
  return ret;
}

/*
static x86_64_Value *visit_int64_literal(x86_64_Visitor *v, int64_t int64_val) {
  x86_64_Value *ret = checked_calloc(1, sizeof(x86_64_Value));
  ret->location_kind = LOC_IMMEDIATE;
  ret->integer_immediate = int64_val;
  ret->debug_name = addr(v, ret);
  ret->type = &INT64_LITERAL_TYPE;
  return ret;
}
*/

static x86_64_Value *visit_float_literal(x86_64_Visitor *v, int64_t int64_val) {
  assert(0 && "Unimplemented!");
}

static x86_64_Value *convert_type(x86_64_Value *value, const Type *new_type) {
  // assert((compare_type(value->type, new_type) != 0) && "Unnecessary convert_type call");
  // handle all the cases later
  x86_64_Value *ret = checked_calloc(1, sizeof(x86_64_Value));

  if (value->location_kind == LOC_IMMEDIATE) {
    // Copy everything
    // TODO: Handle other cases later...
    *ret = *value;
    if (value->type->kind == TY_INTEGER && new_type->kind == TY_FLOAT) {
      value->float_immediate = (double) value->integer_immediate;
    }
  }
  return ret;
}

static void *visit_assign(x86_64_Visitor *v, TokenKind op, x86_64_Value *left, x86_64_Value *right) {
  assert(IS_SCALAR_TYPE(left->type) && IS_SCALAR_TYPE(right->type) && left->type->size == right->type->size);
  int size = left->type->size;
  if (right->location_kind == LOC_IMMEDIATE) {
    fprintf(v->out,
      BINARY_TEMPLATE,
      operator("mov", size), addr(v, right), addr(v, left),
      left->debug_name, right->debug_name
    );
  } else {
    fprintf(v->out, "\t%s\t%s, %s\n", operator("mov", size), addr(v, right), param_registers[size][0]);
    fprintf(v->out, "\t%s\t%s, %s", operator("mov", size), param_registers[size][0], addr(v, left));
    fprintf(v->out, "\t\t# %s = %s\n", left->debug_name, right->debug_name);
  }
  return left;
}

static void *visit_binop(x86_64_Visitor *v, TokenKind op, x86_64_Value *left, x86_64_Value *right) {
  if (!left || !right) {
    raise(SIGTRAP);
  }
  assert(IS_SCALAR_TYPE(left->type) && IS_SCALAR_TYPE(right->type));
  const Type *type = left->type;
  int size = type->size;
  const char *accum_reg = accum_register(size);
  switch (op) {
    case TOK_ADD_OP:
      copy_to_accum(v, left);
      fprintf(
        v->out,
        BINARY_TEMPLATE,
        operator("add", size), addr(v, right), accum_reg,
        accum_reg, fmtstr("%s + %s", left->debug_name, right->debug_name)
      );
      break;
    case TOK_SUB_OP:
      copy_to_accum(v, left);
      fprintf(
        v->out,
        BINARY_TEMPLATE,
        operator("sub", size), addr(v, right), accum_reg,
        accum_reg, fmtstr("%s - %s", left->debug_name, right->debug_name)
      );
      break;
    case TOK_STAR_OP:
      copy_to_accum(v, left);
      fprintf(
        v->out,
        BINARY_TEMPLATE,
        operator("imul", size), addr(v, right), accum_reg,
        accum_reg, fmtstr("%s * %s", left->debug_name, right->debug_name)
      );
      break;
    case TOK_DIV_OP:
      copy_to_accum(v, left);
      char *ct;
      switch (size) {
        case 1: ct = "cbw"; break;
        case 2: ct = "cwd"; break;
        case 4: ct = "cdq"; break;
        case 8: ct = "cqo"; break;
        default: THROWF(EXC_INTERNAL, "wrong size for division: %d", size);
      }
      fprintf(v->out, "\t%s\n", ct);
      fprintf(
        v->out,
        "\t%s\t%s\t\t# %s = %s\n",
        operator("idiv", size), addr(v, right),
        accum_reg,
        fmtstr("%s / %s", left->debug_name, right->debug_name)
      );
      break;
    case TOK_COMMA:
      // special case; just return right
      return right;
    default:
      THROWF(EXC_INTERNAL, "Binop %s not supported", TOKEN_NAMES[op]);
  }
  x86_64_Value *result = new_variable(v, type, NULL);
  copy_from_accum(v, result);
  return result;
}

x86_64_Value *visit_conditional(x86_64_Visitor *v, TokenKind op, int jump, void *left, void *right) {
  assert(0 && "Unimplemented!");
}

// http://6.s081.scripts.mit.edu/sp18/x86-64-architecture-guide.html
static char *prologue = "\t.globl	_%s\n_%s:\n\tpushq\t%%rbp\n\tmovq\t%%rsp, %%rbp\n";
static void visit_function_definition_start(
  x86_64_Visitor *v,
  const char *ident
) {
  v->flags_contents = 0;
  v->accum_contents = 0;
  v->curr_rbp_offset = 0;
  v->curr_temp_id = 0;
  v->curr_func_param = 0;
  v->curr_func_return_type = 0;
  fprintf(v->out, prologue, ident, ident);
}

static void *visit_declaration(
  x86_64_Visitor *v,
  const Type *type,
  const char *ident_string
) {
  x86_64_Value *ret = new_variable(v, type, ident_string);
  return ret;
}

static void *visit_function_definition_param(
  x86_64_Visitor *v,
  const Type *type,
  const char *ident_string
) {
  x86_64_Value *ret = visit_declaration(v, type, ident_string);
  int size = ret->type->size;
  fprintf(
    v->out,
    "\t%s\t%s, %s\n",
    operator("mov", size),
    param_registers[size][v->curr_func_param],
    addr(v, ret)
  );
  v->curr_func_param++;
  return ret;
}

static x86_64_Value *visit_array_reference_base(x86_64_Visitor *v, x86_64_Value *array, x86_64_Value *index, int lvalue) {
  assert(lvalue == 1 && "that's all we do for now");
  assert(array->location_kind != LOC_INDEXED);
  x86_64_Value *ret = checked_calloc(1, sizeof(x86_64_Value));
  ret->location_kind = LOC_INDEXED;
  ret->type = array->type->child_type;
  ret->index.base = array;
  ret->debug_name = fmtstr("%s[%s]", array->debug_name, index->debug_name);

  int element_size = total_size(array->type->child_type);
  int is_scaled = element_size == 1 || element_size == 2 || element_size == 4 || element_size == 8;
  ret->index.scale = is_scaled ? element_size : 0;

  if (index->location_kind == LOC_IMMEDIATE) {
    assert(index->type->kind == TY_INTEGER);
    ret->index.index_const = index->integer_immediate;
    ret->index.index_expr = 0;
    if (!is_scaled) {
      fprintf(stderr, "DEBUG -- not scaled hit!\n");
      ret->index.index_const *= element_size;
    }
  } else if (is_scaled) {  // not a constant index, but constant scale => do not emit code to rescale
    ret->index.index_expr = index;
  } else {  // neither constant index nor constant scale => emit code to rescale
    ret->index.index_expr = visit_binop(v, TOK_STAR_OP, index, visit_integer_literal(v, element_size)); 
  }
  return ret;
}

// TODO: specialize for constant references
static x86_64_Value *visit_array_reference(x86_64_Visitor *v, x86_64_Value *left, x86_64_Value *index, int lvalue) {
  assert(lvalue == 1 && "that's all we do for now");
  if (left->location_kind != LOC_INDEXED) {
    return visit_array_reference_base(v, left, index, lvalue);
  }

  x86_64_Value *ret = checked_calloc(1, sizeof(x86_64_Value));
  checked_memcpy(ret, left, sizeof(x86_64_Value));

  // recursive case: update the index
  // first, adjust type to be the child type, since we are one level down
  ret->type = ret->type->child_type;
  THROW_IF(!ret->type, EXC_PARSE_SYNTAX, "More levels of array references than dimensions of array.");
  ret->debug_name = fmtstr("%s[%s]", ret->debug_name, index->debug_name);
  int element_size = total_size(left->type->child_type);

  /* Let a = T[M][N] and S = N * sizeof(T). Then
   *   &a[i][j] = a + (i * S) + (j * sizeof(T))
   *
   * NOTE: -d(r1,r2,s) = *(r1 - d + r2*s)
   *
   * The only special case is if left was scaled, which would only happen for very obscure cases (i.e. 8 bytes)
   */

  // descale it; x86 special case no longer applies to nested references
  if (IS_SCALED_INDEX(ret->index)) {
    if (IS_CONST_INDEX(ret->index)) {
      ret->index.index_const *= ret->index.scale;  
    } else {
      ret->index.index_expr = visit_binop(
        v,
        TOK_STAR_OP,
        ret->index.index_expr,
        visit_integer_literal(v, ret->index.scale)
      );
    }
    ret->index.scale = 0;
  }

  // Now add the new offset. Note that since array referencing proceeds from left to right, as soon as we lose one
  // const index, subsequent indices are also no longer const.
  assert(!IS_SCALED_INDEX(ret->index));
  if (IS_CONST_INDEX(ret->index) && index->location_kind == LOC_IMMEDIATE) {
    ret->index.index_const += index->integer_immediate * element_size;
  } else {
    x86_64_Value *scaled_index = visit_binop(v, TOK_STAR_OP, index, visit_integer_literal(v, element_size));
    ret->index.index_expr = visit_binop(
      v,
      TOK_ADD_OP,
      scaled_index,
      IS_CONST_INDEX(ret->index) ? visit_integer_literal(v, ret->index.index_const) : ret->index.index_expr
    );
    ret->index.index_const = -1;
  }
  return ret;
}

static x86_64_Value *visit_struct_reference_base(x86_64_Visitor *v, x86_64_Value *left, const Member *member) {
  assert(left->location_kind != LOC_INDEXED);
  x86_64_Value *ret = checked_calloc(1, sizeof(x86_64_Value));
  ret->type = member->type;
  ret->location_kind = LOC_INDEXED;
  ret->index.base = left;
  ret->index.index_const = member->offset;
  ret->debug_name = fmtstr("%s.%s", left->debug_name, member->ident);
  return ret;
}

static x86_64_Value *visit_struct_reference(x86_64_Visitor *v, x86_64_Value *left, const Member *member) {
  if (left->location_kind != LOC_INDEXED) {
    return visit_struct_reference_base(v, left, member);
  }
  assert(0 && "Unimplemented!");
}

static void visit_return(x86_64_Visitor *v, x86_64_Value *retval) {
  copy_to_accum(v, retval);
  fputs("\tleave\n\tretq\n", v->out);
}

static void visit_zero_object(x86_64_Visitor *v, x86_64_Value *object) {
  assert(!IS_SCALAR_TYPE(object->type));
  void *size = visit_integer_literal(v, total_size(object->type));
  fprintf(v->out, "\tleaq\t%s, %%rdi\n", addr(v, object));
  fprintf(v->out, "\txorl\t%%esi, %%esi\n");
  fprintf(v->out, "\tmovl\t%s, %%edx\n", addr(v, size));
  fprintf(v->out, "\tcallq\t_memset\n");
}

static void visit_assign_offset(x86_64_Visitor *v, x86_64_Value *aggregate, int offset, x86_64_Value *right) {
  assert(IS_SCALAR_TYPE(right->type));
  int size = right->type->size;
  assert(aggregate->location_kind == LOC_STACK);
  int total_rbp_offset = aggregate->rbp_offset + offset;

  const char *src;
  if (right->location_kind == LOC_IMMEDIATE) {
    src = addr(v, right);
  } else {
    copy_to_accum(v, right);
    src = accum_register(size);
  }
  fprintf(
    v->out,
    "\t%s\t%s,%d(%%rbp)\t\t# %s[..%d] = %s\n",
    operator("mov", size), src, total_rbp_offset,
    aggregate->debug_name, offset, right->debug_name
  );
}

static void visit_function_end(x86_64_Visitor *v) {
  fputs("\tleave\n\tretq\n", v->out);
}

static void finalize(x86_64_Visitor *v) {
  checked_fclose(v->out);
}

static void emit_comment(x86_64_Visitor *v, const char *fmt, ...) {
  va_list ap;
  va_start(ap, fmt);
  fprintf(v->out, "# ");
  vfprintf(v->out, fmt, ap);
  fprintf(v->out, "\n");
}

Visitor *new_x86_64_visitor(FILE *out) {
  x86_64_Visitor *v = checked_calloc(1, sizeof(x86_64_Visitor));
  INSTALL_VISITOR_METHODS(v)

  // Create the primitive types
  Visitor *super = (Visitor *) v;
  super->char_type        = (Type) { .kind = TY_INTEGER, .size = 1,  .align = 1  };
  super->short_type       = (Type) { .kind = TY_INTEGER, .size = 2,  .align = 2  };
  super->int_type         = (Type) { .kind = TY_INTEGER, .size = 4,  .align = 4  };
  super->long_type        = (Type) { .kind = TY_INTEGER, .size = 8,  .align = 8  };
  super->long_long_type   = (Type) { .kind = TY_INTEGER, .size = 8,  .align = 8  };
  super->float_type       = (Type) { .kind = TY_FLOAT,   .size = 4,  .align = 4  };
  super->double_type      = (Type) { .kind = TY_FLOAT,   .size = 8,  .align = 8  };
  super->long_double_type = (Type) { .kind = TY_FLOAT,   .size = 16, .align = 16 };
  MAKE_ALL_UNSIGNED_TYPES(super);

  v->curr_rbp_offset = 0;
  v->curr_temp_id = 0;
  v->out = out;
  time_t curr_time = time(0);
  char timebuf[27];
  ctime_r(&curr_time, timebuf);
  fprintf(v->out, "# KUI'S COMPILER at %s", timebuf);
  return (Visitor *) v;
}
